{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10ed8cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement constractions (from versions: none)\n",
      "ERROR: No matching distribution found for constractions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contractions in c:\\users\\admin\\anaconda3\\lib\\site-packages (0.1.73)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from contractions) (0.0.24)\n",
      "Requirement already satisfied: pyahocorasick in c:\\users\\admin\\anaconda3\\lib\\site-packages (from textsearch>=0.0.21->contractions) (2.0.0)\n",
      "Requirement already satisfied: anyascii in c:\\users\\admin\\anaconda3\\lib\\site-packages (from textsearch>=0.0.21->contractions) (0.3.1)\n",
      "Requirement already satisfied: unidecode in c:\\users\\admin\\anaconda3\\lib\\site-packages (1.2.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install constractions\n",
    "import sys  \n",
    "!{sys.executable} -m pip install contractions\n",
    "\n",
    "import contractions\n",
    "! pip install unidecode\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "232eae36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize,WhitespaceTokenizer\n",
    "from nltk.stem import WordNetLemmatizer, LancasterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from unidecode import unidecode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f6736a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"The Moon is a barren, rocky world without air and water.\n",
    "It has dark lava plain on its surface. The Moon is filled wit craters. \n",
    "It has no light of its own. It gets its light from the Sun. The Moo keeps\n",
    "changing its shape as it moves round the Earth. It spins on its axis in 27.3 days\n",
    "stars were named after the Edwin Aldrin were the first ones to set their foot on the Moon\n",
    "on 21 July 1969 They reached the Moon in their space craft named Apollo II.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19b0972b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Moon is a barren, rocky world without air and water.',\n",
       " 'It has dark lava plain on its surface.',\n",
       " 'The Moon is filled wit craters.',\n",
       " 'It has no light of its own.',\n",
       " 'It gets its light from the Sun.',\n",
       " 'The Moo keeps\\nchanging its shape as it moves round the Earth.',\n",
       " 'It spins on its axis in 27.3 days\\nstars were named after the Edwin Aldrin were the first ones to set their foot on the Moon\\non 21 July 1969 They reached the Moon in their space craft named Apollo II.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenization\n",
    "# sentence tokenization\n",
    "sent = sent_tokenize(text)\n",
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "394068b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Moon',\n",
       " 'is',\n",
       " 'a',\n",
       " 'barren',\n",
       " ',',\n",
       " 'rocky',\n",
       " 'world',\n",
       " 'without',\n",
       " 'air',\n",
       " 'and',\n",
       " 'water',\n",
       " '.',\n",
       " 'It',\n",
       " 'has',\n",
       " 'dark',\n",
       " 'lava',\n",
       " 'plain',\n",
       " 'on',\n",
       " 'its',\n",
       " 'surface',\n",
       " '.',\n",
       " 'The',\n",
       " 'Moon',\n",
       " 'is',\n",
       " 'filled',\n",
       " 'wit',\n",
       " 'craters',\n",
       " '.',\n",
       " 'It',\n",
       " 'has',\n",
       " 'no',\n",
       " 'light',\n",
       " 'of',\n",
       " 'its',\n",
       " 'own',\n",
       " '.',\n",
       " 'It',\n",
       " 'gets',\n",
       " 'its',\n",
       " 'light',\n",
       " 'from',\n",
       " 'the',\n",
       " 'Sun',\n",
       " '.',\n",
       " 'The',\n",
       " 'Moo',\n",
       " 'keeps',\n",
       " 'changing',\n",
       " 'its',\n",
       " 'shape',\n",
       " 'as',\n",
       " 'it',\n",
       " 'moves',\n",
       " 'round',\n",
       " 'the',\n",
       " 'Earth',\n",
       " '.',\n",
       " 'It',\n",
       " 'spins',\n",
       " 'on',\n",
       " 'its',\n",
       " 'axis',\n",
       " 'in',\n",
       " '27.3',\n",
       " 'days',\n",
       " 'stars',\n",
       " 'were',\n",
       " 'named',\n",
       " 'after',\n",
       " 'the',\n",
       " 'Edwin',\n",
       " 'Aldrin',\n",
       " 'were',\n",
       " 'the',\n",
       " 'first',\n",
       " 'ones',\n",
       " 'to',\n",
       " 'set',\n",
       " 'their',\n",
       " 'foot',\n",
       " 'on',\n",
       " 'the',\n",
       " 'Moon',\n",
       " 'on',\n",
       " '21',\n",
       " 'July',\n",
       " '1969',\n",
       " 'They',\n",
       " 'reached',\n",
       " 'the',\n",
       " 'Moon',\n",
       " 'in',\n",
       " 'their',\n",
       " 'space',\n",
       " 'craft',\n",
       " 'named',\n",
       " 'Apollo',\n",
       " 'II',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word tokenization\n",
    "tokens = word_tokenize(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe228e79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Moon',\n",
       " 'is',\n",
       " 'a',\n",
       " 'barren,',\n",
       " 'rocky',\n",
       " 'world',\n",
       " 'without',\n",
       " 'air',\n",
       " 'and',\n",
       " 'water.',\n",
       " 'It',\n",
       " 'has',\n",
       " 'dark',\n",
       " 'lava',\n",
       " 'plain',\n",
       " 'on',\n",
       " 'its',\n",
       " 'surface.',\n",
       " 'The',\n",
       " 'Moon',\n",
       " 'is',\n",
       " 'filled',\n",
       " 'wit',\n",
       " 'craters.',\n",
       " 'It',\n",
       " 'has',\n",
       " 'no',\n",
       " 'light',\n",
       " 'of',\n",
       " 'its',\n",
       " 'own.',\n",
       " 'It',\n",
       " 'gets',\n",
       " 'its',\n",
       " 'light',\n",
       " 'from',\n",
       " 'the',\n",
       " 'Sun.',\n",
       " 'The',\n",
       " 'Moo',\n",
       " 'keeps',\n",
       " 'changing',\n",
       " 'its',\n",
       " 'shape',\n",
       " 'as',\n",
       " 'it',\n",
       " 'moves',\n",
       " 'round',\n",
       " 'the',\n",
       " 'Earth.',\n",
       " 'It',\n",
       " 'spins',\n",
       " 'on',\n",
       " 'its',\n",
       " 'axis',\n",
       " 'in',\n",
       " '27.3',\n",
       " 'days',\n",
       " 'stars',\n",
       " 'were',\n",
       " 'named',\n",
       " 'after',\n",
       " 'the',\n",
       " 'Edwin',\n",
       " 'Aldrin',\n",
       " 'were',\n",
       " 'the',\n",
       " 'first',\n",
       " 'ones',\n",
       " 'to',\n",
       " 'set',\n",
       " 'their',\n",
       " 'foot',\n",
       " 'on',\n",
       " 'the',\n",
       " 'Moon',\n",
       " 'on',\n",
       " '21',\n",
       " 'July',\n",
       " '1969',\n",
       " 'They',\n",
       " 'reached',\n",
       " 'the',\n",
       " 'Moon',\n",
       " 'in',\n",
       " 'their',\n",
       " 'space',\n",
       " 'craft',\n",
       " 'named',\n",
       " 'Apollo',\n",
       " 'II.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# whitespace tokenization\n",
    "tokens1 = WhitespaceTokenizer().tokenize(text)\n",
    "tokens1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd521580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'moon',\n",
       " 'is',\n",
       " 'a',\n",
       " 'barren',\n",
       " ',',\n",
       " 'rocky',\n",
       " 'world',\n",
       " 'without',\n",
       " 'air',\n",
       " 'and',\n",
       " 'water',\n",
       " '.',\n",
       " 'it',\n",
       " 'has',\n",
       " 'dark',\n",
       " 'lava',\n",
       " 'plain',\n",
       " 'on',\n",
       " 'its',\n",
       " 'surface',\n",
       " '.',\n",
       " 'the',\n",
       " 'moon',\n",
       " 'is',\n",
       " 'filled',\n",
       " 'wit',\n",
       " 'craters',\n",
       " '.',\n",
       " 'it',\n",
       " 'has',\n",
       " 'no',\n",
       " 'light',\n",
       " 'of',\n",
       " 'its',\n",
       " 'own',\n",
       " '.',\n",
       " 'it',\n",
       " 'gets',\n",
       " 'its',\n",
       " 'light',\n",
       " 'from',\n",
       " 'the',\n",
       " 'sun',\n",
       " '.',\n",
       " 'the',\n",
       " 'moo',\n",
       " 'keeps',\n",
       " 'changing',\n",
       " 'its',\n",
       " 'shape',\n",
       " 'as',\n",
       " 'it',\n",
       " 'moves',\n",
       " 'round',\n",
       " 'the',\n",
       " 'earth',\n",
       " '.',\n",
       " 'it',\n",
       " 'spins',\n",
       " 'on',\n",
       " 'its',\n",
       " 'axis',\n",
       " 'in',\n",
       " '27.3',\n",
       " 'days',\n",
       " 'stars',\n",
       " 'were',\n",
       " 'named',\n",
       " 'after',\n",
       " 'the',\n",
       " 'edwin',\n",
       " 'aldrin',\n",
       " 'were',\n",
       " 'the',\n",
       " 'first',\n",
       " 'ones',\n",
       " 'to',\n",
       " 'set',\n",
       " 'their',\n",
       " 'foot',\n",
       " 'on',\n",
       " 'the',\n",
       " 'moon',\n",
       " 'on',\n",
       " '21',\n",
       " 'july',\n",
       " '1969',\n",
       " 'they',\n",
       " 'reached',\n",
       " 'the',\n",
       " 'moon',\n",
       " 'in',\n",
       " 'their',\n",
       " 'space',\n",
       " 'craft',\n",
       " 'named',\n",
       " 'apollo',\n",
       " 'ii',\n",
       " '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalization\n",
    "lower_text = [word.lower() for word in tokens]\n",
    "lower_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c1970cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa01a359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'moon',\n",
       " 'is',\n",
       " 'a',\n",
       " 'barren',\n",
       " 'rocky',\n",
       " 'world',\n",
       " 'without',\n",
       " 'air',\n",
       " 'and',\n",
       " 'water',\n",
       " 'it',\n",
       " 'has',\n",
       " 'dark',\n",
       " 'lava',\n",
       " 'plain',\n",
       " 'on',\n",
       " 'its',\n",
       " 'surface',\n",
       " 'the',\n",
       " 'moon',\n",
       " 'is',\n",
       " 'filled',\n",
       " 'wit',\n",
       " 'craters',\n",
       " 'it',\n",
       " 'has',\n",
       " 'no',\n",
       " 'light',\n",
       " 'of',\n",
       " 'its',\n",
       " 'own',\n",
       " 'it',\n",
       " 'gets',\n",
       " 'its',\n",
       " 'light',\n",
       " 'from',\n",
       " 'the',\n",
       " 'sun',\n",
       " 'the',\n",
       " 'moo',\n",
       " 'keeps',\n",
       " 'changing',\n",
       " 'its',\n",
       " 'shape',\n",
       " 'as',\n",
       " 'it',\n",
       " 'moves',\n",
       " 'round',\n",
       " 'the',\n",
       " 'earth',\n",
       " 'it',\n",
       " 'spins',\n",
       " 'on',\n",
       " 'its',\n",
       " 'axis',\n",
       " 'in',\n",
       " '27.3',\n",
       " 'days',\n",
       " 'stars',\n",
       " 'were',\n",
       " 'named',\n",
       " 'after',\n",
       " 'the',\n",
       " 'edwin',\n",
       " 'aldrin',\n",
       " 'were',\n",
       " 'the',\n",
       " 'first',\n",
       " 'ones',\n",
       " 'to',\n",
       " 'set',\n",
       " 'their',\n",
       " 'foot',\n",
       " 'on',\n",
       " 'the',\n",
       " 'moon',\n",
       " 'on',\n",
       " '21',\n",
       " 'july',\n",
       " '1969',\n",
       " 'they',\n",
       " 'reached',\n",
       " 'the',\n",
       " 'moon',\n",
       " 'in',\n",
       " 'their',\n",
       " 'space',\n",
       " 'craft',\n",
       " 'named',\n",
       " 'apollo',\n",
       " 'ii']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove punctuations\n",
    "text_without_punct = [word for word in lower_text if word not in punctuation]\n",
    "text_without_punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e6260d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove stopwords\n",
    "stopword_list = stopwords.words('english')\n",
    "len(stopword_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54f82294",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['moon',\n",
       " 'barren',\n",
       " 'rocky',\n",
       " 'world',\n",
       " 'without',\n",
       " 'air',\n",
       " 'water',\n",
       " 'dark',\n",
       " 'lava',\n",
       " 'plain',\n",
       " 'surface',\n",
       " 'moon',\n",
       " 'filled',\n",
       " 'wit',\n",
       " 'craters',\n",
       " 'light',\n",
       " 'gets',\n",
       " 'light',\n",
       " 'sun',\n",
       " 'moo',\n",
       " 'keeps',\n",
       " 'changing',\n",
       " 'shape',\n",
       " 'moves',\n",
       " 'round',\n",
       " 'earth',\n",
       " 'spins',\n",
       " 'axis',\n",
       " '27.3',\n",
       " 'days',\n",
       " 'stars',\n",
       " 'named',\n",
       " 'edwin',\n",
       " 'aldrin',\n",
       " 'first',\n",
       " 'ones',\n",
       " 'set',\n",
       " 'foot',\n",
       " 'moon',\n",
       " '21',\n",
       " 'july',\n",
       " '1969',\n",
       " 'reached',\n",
       " 'moon',\n",
       " 'space',\n",
       " 'craft',\n",
       " 'named',\n",
       " 'apollo',\n",
       " 'ii']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_without_stop = [word for word in text_without_punct if word not in stopword_list]\n",
    "text_without_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92dbcfdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'moon', 'is', 'a', 'barren', 'rocky', 'world', 'without', 'air', 'and']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_without_punct[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "413355d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['moon',\n",
       " 'barren',\n",
       " 'rocky',\n",
       " 'world',\n",
       " 'without',\n",
       " 'air',\n",
       " 'water',\n",
       " 'dark',\n",
       " 'lava',\n",
       " 'plain']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_without_stop[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a60e26fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : Hello mom! Yes, I'm fine. How're you? No, I didn't have lunch. I'm about to go.\n",
      "Are you coming next weekend? I've been missing you.\n",
      "\n",
      "\n",
      "Output: Hello mom! Yes, I am fine. How are you? No, I did not have lunch. I am about to go. Are you coming next weekend? I have been missing you.\n"
     ]
    }
   ],
   "source": [
    "import contractions\n",
    "\n",
    "text = '''Hello mom! Yes, I'm fine. How're you? No, I didn't have lunch. I'm about to go.\n",
    "Are you coming next weekend? I've been missing you.'''\n",
    " \n",
    "expanded_text = []   \n",
    "for word in text.split():\n",
    "  expanded_text.append(contractions.fix(word))  \n",
    "   \n",
    "expanded_text = ' '.join(expanded_text)\n",
    "print('Input : ' + text)\n",
    "print('\\n')\n",
    "print('Output: ' + expanded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6724d744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I did not like the movie'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# contraction mapping \n",
    "text1 = \"I didn't like the movie\"\n",
    "expanded_text = contractions.fix(text1)\n",
    "expanded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ac4dae9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I does not like the movie'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# contraction mapping \n",
    "text1 = \"I doesn't like the movie\"\n",
    "expanded_text = contractions.fix(text1)\n",
    "expanded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d94d3c89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I cannot like the movie'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# contraction mapping \n",
    "text1 = \"I can't like the movie\"\n",
    "expanded_text = contractions.fix(text1)\n",
    "expanded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "891e5634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# {can't:can not}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c7c567d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original word >> moon\n",
      "stemmed_word >> moon\n",
      "lemmatized_word >> moon\n",
      "****************************************************************************************************\n",
      "Original word >> barren\n",
      "stemmed_word >> bar\n",
      "lemmatized_word >> barren\n",
      "****************************************************************************************************\n",
      "Original word >> rocky\n",
      "stemmed_word >> rocky\n",
      "lemmatized_word >> rocky\n",
      "****************************************************************************************************\n",
      "Original word >> world\n",
      "stemmed_word >> world\n",
      "lemmatized_word >> world\n",
      "****************************************************************************************************\n",
      "Original word >> without\n",
      "stemmed_word >> without\n",
      "lemmatized_word >> without\n",
      "****************************************************************************************************\n",
      "Original word >> air\n",
      "stemmed_word >> air\n",
      "lemmatized_word >> air\n",
      "****************************************************************************************************\n",
      "Original word >> water\n",
      "stemmed_word >> wat\n",
      "lemmatized_word >> water\n",
      "****************************************************************************************************\n",
      "Original word >> dark\n",
      "stemmed_word >> dark\n",
      "lemmatized_word >> dark\n",
      "****************************************************************************************************\n",
      "Original word >> lava\n",
      "stemmed_word >> lav\n",
      "lemmatized_word >> lava\n",
      "****************************************************************************************************\n",
      "Original word >> plain\n",
      "stemmed_word >> plain\n",
      "lemmatized_word >> plain\n",
      "****************************************************************************************************\n",
      "Original word >> surface\n",
      "stemmed_word >> surfac\n",
      "lemmatized_word >> surface\n",
      "****************************************************************************************************\n",
      "Original word >> moon\n",
      "stemmed_word >> moon\n",
      "lemmatized_word >> moon\n",
      "****************************************************************************************************\n",
      "Original word >> filled\n",
      "stemmed_word >> fil\n",
      "lemmatized_word >> filled\n",
      "****************************************************************************************************\n",
      "Original word >> wit\n",
      "stemmed_word >> wit\n",
      "lemmatized_word >> wit\n",
      "****************************************************************************************************\n",
      "Original word >> craters\n",
      "stemmed_word >> crat\n",
      "lemmatized_word >> crater\n",
      "****************************************************************************************************\n",
      "Original word >> light\n",
      "stemmed_word >> light\n",
      "lemmatized_word >> light\n",
      "****************************************************************************************************\n",
      "Original word >> gets\n",
      "stemmed_word >> get\n",
      "lemmatized_word >> get\n",
      "****************************************************************************************************\n",
      "Original word >> light\n",
      "stemmed_word >> light\n",
      "lemmatized_word >> light\n",
      "****************************************************************************************************\n",
      "Original word >> sun\n",
      "stemmed_word >> sun\n",
      "lemmatized_word >> sun\n",
      "****************************************************************************************************\n",
      "Original word >> moo\n",
      "stemmed_word >> moo\n",
      "lemmatized_word >> moo\n",
      "****************************************************************************************************\n",
      "Original word >> keeps\n",
      "stemmed_word >> keep\n",
      "lemmatized_word >> keep\n",
      "****************************************************************************************************\n",
      "Original word >> changing\n",
      "stemmed_word >> chang\n",
      "lemmatized_word >> changing\n",
      "****************************************************************************************************\n",
      "Original word >> shape\n",
      "stemmed_word >> shap\n",
      "lemmatized_word >> shape\n",
      "****************************************************************************************************\n",
      "Original word >> moves\n",
      "stemmed_word >> mov\n",
      "lemmatized_word >> move\n",
      "****************************************************************************************************\n",
      "Original word >> round\n",
      "stemmed_word >> round\n",
      "lemmatized_word >> round\n",
      "****************************************************************************************************\n",
      "Original word >> earth\n",
      "stemmed_word >> ear\n",
      "lemmatized_word >> earth\n",
      "****************************************************************************************************\n",
      "Original word >> spins\n",
      "stemmed_word >> spin\n",
      "lemmatized_word >> spin\n",
      "****************************************************************************************************\n",
      "Original word >> axis\n",
      "stemmed_word >> ax\n",
      "lemmatized_word >> axis\n",
      "****************************************************************************************************\n",
      "Original word >> 27.3\n",
      "stemmed_word >> 27.3\n",
      "lemmatized_word >> 27.3\n",
      "****************************************************************************************************\n",
      "Original word >> days\n",
      "stemmed_word >> day\n",
      "lemmatized_word >> day\n",
      "****************************************************************************************************\n",
      "Original word >> stars\n",
      "stemmed_word >> star\n",
      "lemmatized_word >> star\n",
      "****************************************************************************************************\n",
      "Original word >> named\n",
      "stemmed_word >> nam\n",
      "lemmatized_word >> named\n",
      "****************************************************************************************************\n",
      "Original word >> edwin\n",
      "stemmed_word >> edwin\n",
      "lemmatized_word >> edwin\n",
      "****************************************************************************************************\n",
      "Original word >> aldrin\n",
      "stemmed_word >> aldrin\n",
      "lemmatized_word >> aldrin\n",
      "****************************************************************************************************\n",
      "Original word >> first\n",
      "stemmed_word >> first\n",
      "lemmatized_word >> first\n",
      "****************************************************************************************************\n",
      "Original word >> ones\n",
      "stemmed_word >> on\n",
      "lemmatized_word >> one\n",
      "****************************************************************************************************\n",
      "Original word >> set\n",
      "stemmed_word >> set\n",
      "lemmatized_word >> set\n",
      "****************************************************************************************************\n",
      "Original word >> foot\n",
      "stemmed_word >> foot\n",
      "lemmatized_word >> foot\n",
      "****************************************************************************************************\n",
      "Original word >> moon\n",
      "stemmed_word >> moon\n",
      "lemmatized_word >> moon\n",
      "****************************************************************************************************\n",
      "Original word >> 21\n",
      "stemmed_word >> 21\n",
      "lemmatized_word >> 21\n",
      "****************************************************************************************************\n",
      "Original word >> july\n",
      "stemmed_word >> july\n",
      "lemmatized_word >> july\n",
      "****************************************************************************************************\n",
      "Original word >> 1969\n",
      "stemmed_word >> 1969\n",
      "lemmatized_word >> 1969\n",
      "****************************************************************************************************\n",
      "Original word >> reached\n",
      "stemmed_word >> reach\n",
      "lemmatized_word >> reached\n",
      "****************************************************************************************************\n",
      "Original word >> moon\n",
      "stemmed_word >> moon\n",
      "lemmatized_word >> moon\n",
      "****************************************************************************************************\n",
      "Original word >> space\n",
      "stemmed_word >> spac\n",
      "lemmatized_word >> space\n",
      "****************************************************************************************************\n",
      "Original word >> craft\n",
      "stemmed_word >> craft\n",
      "lemmatized_word >> craft\n",
      "****************************************************************************************************\n",
      "Original word >> named\n",
      "stemmed_word >> nam\n",
      "lemmatized_word >> named\n",
      "****************************************************************************************************\n",
      "Original word >> apollo\n",
      "stemmed_word >> apollo\n",
      "lemmatized_word >> apollo\n",
      "****************************************************************************************************\n",
      "Original word >> ii\n",
      "stemmed_word >> ii\n",
      "lemmatized_word >> ii\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "stemming = LancasterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for word in text_without_stop:\n",
    "    stemmed_word = stemming.stem(word)\n",
    "    lemmatized_word = lemmatizer.lemmatize(word)\n",
    "    print(f'Original word >> {word}')\n",
    "    print(f'stemmed_word >> {stemmed_word}')\n",
    "    print(f'lemmatized_word >> {lemmatized_word}')\n",
    "    print('*'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c67e3739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a, A, a, A, E'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Handling accented characters\n",
    "accented_character = \"á, À, ä, Å, É\"\n",
    "fixed_words = unidecode(accented_character)\n",
    "fixed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b76d39bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'textblob'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtextblob\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TextBlob\n\u001b[0;32m      2\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmussage\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      3\u001b[0m b \u001b[38;5;241m=\u001b[39m TextBlob(a)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'textblob'"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "a = 'mussage'\n",
    "b = TextBlob(a)\n",
    "print(b.correct())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629e74f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autocorrect import Speller\n",
    "spell = Speller(lang='en')\n",
    "print(spell('mussage'))\n",
    "print(spell('survice'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430a578d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
